{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp9Bd99_0wln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe49f7b7-cf04-4c51-c8bc-71d03a776a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.utils.data as Dataloader\n",
        "from torchvision import transforms,datasets\n",
        "from torch import nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels:int=3,\n",
        "               patch_size:int=16,\n",
        "               embedding_dim:int=768) :\n",
        "      super().__init__()\n",
        "\n",
        "      self.patcher = nn.Conv2d(in_channels = in_channels,\n",
        "                               out_channels = embedding_dim,\n",
        "                               kernel_size = patch_size,\n",
        "                               stride = patch_size,\n",
        "                               padding=0)\n",
        "      self.flatten = nn.Flatten(start_dim=2,end_dim=3)\n",
        "\n",
        "  def forward(self,x) :\n",
        "    # image to patches\n",
        "    print(x.size())\n",
        "    x = self.patcher(x)\n",
        "    print(x.size())\n",
        "    # flatten the patches\n",
        "    x_flattened = self.flatten(x)\n",
        "    # permute to batch_size * num_patches * embedding_dim\n",
        "    return x_flattened.permute(0,2,1)\n"
      ],
      "metadata": {
        "id": "-kzxLYY01dWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embed_dim:int=768,\n",
        "               num_heads:int=12,\n",
        "               att_dropout:float=0,\n",
        "               ):\n",
        "    super().__init__()\n",
        "    # create LayerNorm layer\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "    # create Multi Head Self Attention Layer\n",
        "    self.msa = nn.MultiheadAttention(embed_dim = embed_dim,\n",
        "                                     num_heads = num_heads,\n",
        "                                     dropout = att_dropout,\n",
        "                                     batch_first = True)\n",
        "  def forward(self, input):\n",
        "    x = self.layer_norm(input)\n",
        "\n",
        "    att_output,_ = self.msa(query = x,\n",
        "                            key = x,\n",
        "                            value = x,\n",
        "                            need_weights=False)\n",
        "    #skip connection\n",
        "    att_output = att_output + input\n",
        "\n",
        "    return att_output\n",
        "\n"
      ],
      "metadata": {
        "id": "8o4l1O7q_Qwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptronBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embed_dim:int=768,\n",
        "               mlp_size:int=3072,\n",
        "               dropout:float=0.1) :\n",
        "    super().__init__()\n",
        "    # create LayerNorm layer\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "    # create Multi Layer Perceptron layer\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim,mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(mlp_size,embed_dim),\n",
        "        nn.Dropout(p=dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    x = self.layer_norm(input)\n",
        "    x = self.mlp(x)\n",
        "    # skip connection\n",
        "    x = x+input\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "On0oe7hQCwfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embed_dim:int=768,\n",
        "               mlp_size:int=3072,\n",
        "               num_heads:int=12,\n",
        "               att_dropout:float=0.0,\n",
        "               mlp_dropout:float=0.1):\n",
        "    super().__init__()\n",
        "    # Multi Head Self Attentation Block (Layer Norm + Multi Head Self Attentation + Skip Connection)\n",
        "    self.msa = MultiHeadSelfAttentionBlock(embed_dim=embed_dim,\n",
        "                                           num_heads=num_heads,\n",
        "                                           att_dropout=att_dropout)\n",
        "    # Multi layer perceptron Block(Layer Norm + Multi layer perceptron + Skip Connection)\n",
        "    self.mlp = MultiLayerPerceptronBlock(embed_dim=embed_dim,\n",
        "                                         mlp_size=mlp_size,\n",
        "                                         dropout=mlp_dropout)\n",
        "  def forward(self,input):\n",
        "    x = self.msa(input)\n",
        "    x = self.mlp(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "N3iASwmPJxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_block = TransformerEncoderBlock()\n",
        "\n",
        "summary(transformer_block,(1,196,768))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMzjRKfvwY1x",
        "outputId": "10c121ba-c841-436d-adba-4992773ebdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TransformerEncoderBlock                  [1, 196, 768]             --\n",
              "├─MultiHeadSelfAttentionBlock: 1-1       [1, 196, 768]             --\n",
              "│    └─LayerNorm: 2-1                    [1, 196, 768]             1,536\n",
              "│    └─MultiheadAttention: 2-2           [1, 196, 768]             2,362,368\n",
              "├─MultiLayerPerceptronBlock: 1-2         [1, 196, 768]             --\n",
              "│    └─LayerNorm: 2-3                    [1, 196, 768]             1,536\n",
              "│    └─Sequential: 2-4                   [1, 196, 768]             --\n",
              "│    │    └─Linear: 3-1                  [1, 196, 3072]            2,362,368\n",
              "│    │    └─GELU: 3-2                    [1, 196, 3072]            --\n",
              "│    │    └─Dropout: 3-3                 [1, 196, 3072]            --\n",
              "│    │    └─Linear: 3-4                  [1, 196, 768]             2,360,064\n",
              "│    │    └─Dropout: 3-5                 [1, 196, 768]             --\n",
              "==========================================================================================\n",
              "Total params: 7,087,872\n",
              "Trainable params: 7,087,872\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 4.73\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 8.43\n",
              "Params size (MB): 18.90\n",
              "Estimated Total Size (MB): 27.93\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VIT(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               img_size:int=224,\n",
        "               num_classes:int=2,\n",
        "               num_transformer_block:int=12,\n",
        "               in_channels:int=3,\n",
        "               patch_size:int=16,\n",
        "               embed_dim:int=768,\n",
        "               mlp_size:int=3072,\n",
        "               num_heads:int=12,\n",
        "               att_dropout:float=0.0,\n",
        "               mlp_dropout:float=0.1,\n",
        "               embedding_dropout:float=0.0\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {img_size}, patch size: {patch_size}\"\n",
        "\n",
        "    self.num_patches = int((img_size**2)/(patch_size**2))\n",
        "\n",
        "    self.classEmbedding = nn.Parameter(data=torch.rand((1,1,embed_dim)),requires_grad=True)\n",
        "\n",
        "    self.patchEmbedding = PatchEmbedding(in_channels=in_channels,patch_size=patch_size,embedding_dim=embed_dim)\n",
        "\n",
        "    self.positionalEmbedding = nn.Parameter(data=torch.rand((1,self.num_patches+1,embed_dim)),requires_grad=True)\n",
        "\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "    self.transformerEncoder = nn.Sequential(*[TransformerEncoderBlock(embed_dim=embed_dim,\n",
        "                                                                      mlp_size=mlp_size,\n",
        "                                                                      num_heads=num_heads,\n",
        "                                                                      att_dropout=att_dropout,\n",
        "                                                                      mlp_dropout=mlp_dropout) for _ in range(num_transformer_block)])\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embed_dim),\n",
        "        nn.Linear(in_features=embed_dim,out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    batchSize = input.shape[0]\n",
        "    # print(batchSize)\n",
        "    x = self.patchEmbedding(input)\n",
        "    cls_tokens = self.classEmbedding.expand((batchSize,-1,-1))\n",
        "    x = torch.concat((cls_tokens,x),dim=1)\n",
        "    x = self.positionalEmbedding + x\n",
        "    x = self.embedding_dropout(x)\n",
        "    x = self.transformerEncoder(x)\n",
        "    output = self.classifier(x[:,0])\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "88UJagRuNYiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.rand((1,3,224,224))\n",
        "\n",
        "model = VIT()\n",
        "\n",
        "output = model(image)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r_W99Slnqq8",
        "outputId": "3f004269-5ce7-4e96-8e44-54c1cdaf71bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 768, 14, 14])\n",
            "tensor([[ 0.2373, -0.3200]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2rSqKzk2z4N",
        "outputId": "1440d8b2-13d8-4513-ecc0-75d5da9419ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "VIT                                                               152,064\n",
              "├─PatchEmbedding: 1-1                                             --\n",
              "│    └─Conv2d: 2-1                                                590,592\n",
              "│    └─Flatten: 2-2                                               --\n",
              "├─Dropout: 1-2                                                    --\n",
              "├─Sequential: 1-3                                                 --\n",
              "│    └─TransformerEncoderBlock: 2-3                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-1                      2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-2                        4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-4                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-3                      2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-4                        4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-5                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-5                      2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-6                        4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-6                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-7                      2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-8                        4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-7                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-9                      2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-10                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-8                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-11                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-12                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-9                               --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-13                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-14                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-10                              --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-15                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-16                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-11                              --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-17                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-18                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-12                              --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-19                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-20                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-13                              --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-21                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-22                       4,723,968\n",
              "│    └─TransformerEncoderBlock: 2-14                              --\n",
              "│    │    └─MultiHeadSelfAttentionBlock: 3-23                     2,363,904\n",
              "│    │    └─MultiLayerPerceptronBlock: 3-24                       4,723,968\n",
              "├─Sequential: 1-4                                                 --\n",
              "│    └─LayerNorm: 2-15                                            1,536\n",
              "│    └─Linear: 2-16                                               1,538\n",
              "==========================================================================================\n",
              "Total params: 85,800,194\n",
              "Trainable params: 85,800,194\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "p7SCWvj-3k38",
        "outputId": "6bd3587d-dae1-4ba1-aeda-70cfbb744959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tar' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9197f014aeb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtar\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhelp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tar' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bMw9C78iVgr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}